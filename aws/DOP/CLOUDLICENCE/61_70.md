# CloudLicence

## 61.

- 現在の状況の問題点
  - ロードバランサー：1 つのロードバランサーがあります。
  - インスタンス：単一のアベイラビリティゾーン（AZ）に micro インスタンスタイプのインスタンスが 2 から 64 の範囲でスケールします。
  - バックエンド：DynamoDB で構成されています。
  - パフォーマンス問題：顧客がウェブサイトを表示する際に長いロード時間を経験しています。
- 問題の原因
  - インスタンスの性能：micro インスタンスはリソースが少なく、パフォーマンスが低いです。
  - 単一 AZ のリスク：単一の AZ に依存しているため、高可用性が確保されていません。
- インスタンスのサイズ変更：
  - 現状：micro インスタンスを使用しています。
  - 改善：より大きなインスタンス（例えば、t3.medium や t3.large など）に変更します。これにより、1 つのインスタンスあたりのリソースが増え、処理能力が向上します。
- 複数 AZ の利用：

  - 現状：単一の AZ でしかスケーリングしていません。
  - 改善：Auto Scaling グループを設定して、複数の AZ でインスタンスをスケールさせます。これにより、高可用性が確保され、1 つの AZ に障害が発生した場合でも、他の AZ でサービスを継続できます。

- EC2 インスタンスのアイドル状態: インスタンスが起動しているものの、特定のリソース（CPU、メモリ、ディスク I/O、ネットワークなど）がほとんど、またはまったく使用されていない状態を指します。この状態では、インスタンスは稼働中ですが、実際の仕事をしていないか、非常に少ない負荷しかかかっていないことになります。
- AWS Trusted Advisor:

  - 以下のコスト最適化チェックでは、リソースの使用率を検査し、使用率が低いリソースにフラグを付けます。
  - Amazon RDS Idle DB Instances では、Amazon RDS の設定が検査され、アイドル状態の DB インスタンスがあるかどうかが検査されます。過去 7 日間にアクティブな DB インスタンスに接続しなかった場合、このチェック結果の黄色のアイコンが表示されます。スナップショットを作成し、インスタンスを削除または停止できます。
  - Idle Load Balancers は、ELB 設定を検査し、非アクティブなロードバランサーがないか確認します。アクティブに使用されていないロードバランサーがある場合、このチェック結果の黄色のアイコンが表示されます。
  - Low Utilization Amazon EC2 Instances では、使用率が低い EC2 インスタンスを停止または終了することを推奨しています。また、Amazon EC2 Auto Scaling を使用してインスタンスをスケールすることもできます。
  - Unassociated Elastic IP Addresses は、実行中の EC2 インスタンスに関連付けられていない、アカウント内の Elastic IP アドレスを識別します。実行中のインスタンスにアタッチされていない Elastic IP アドレスには、料金が発生します。Elastic IP アドレスを実行中のインスタンスに関連付けるか、解放してコストを削減できます。
  - Underutilized Amazon EBS Volumes は、EBS ボリューム設定で、使用率の低いボリュームがないかどうかを検査します。使用率の低いボリュームのスナップショットを作成し、削除することで、コストを削減できます。 -使用率の低い Amazon Redshift クラスターは、Amazon Redshift の設定をチェックして、使用率の低いクラスターがないか確認します。クラスターのスナップショットを作成して、シャットダウンできます。クラスターのダウンサイジングを選択することもできます。

- Amazon Aurora Global Database: 複数の AWS リージョン にまたがり配置されます。これにより、低レイテンシーのグローバル読み取りを実現し、AWS リージョン 全体に影響が及ぶ可能性のある停止がまれに起きても、すばやい復旧を可能にします。Aurora Global Database には、1 つのリージョンにプライマリ DB クラスターがあり、異なるリージョンに最大 5 つのセカンダリ DB クラスターがあります。単一の Amazon Aurora データベースを複数の AWS リージョンにまたがって運用できます。データベースのパフォーマンスに影響を与えずにデータをレプリケートし、各リージョンでレイテンシーを低減してローカル読み取りを高速化し、リージョン規模の停止からのディザスタリカバリを実現します。

## 62.

- AWS Storage Gateway: オンプレミス環境と AWS クラウド間のストレージを統合するためのハイブリッドクラウドストレージサービスです。これにより、オンプレミスのアプリケーションがクラウドストレージを使用することができます。
  - ![image](https://github.com/yoshikikasama/system/assets/61643054/230fecbc-38f4-4e59-83c8-23b195f11d27)
  - RefreshCache 操作は、ファイルゲートウェイが S3 バケットの内容を最新の状態に保つためにキャッシュを更新する操作です。これにより、S3 バケット内の新しいオブジェクトや変更されたオブジェクトがファイルゲートウェイ経由でユーザーに見えるようになります。
  - ファイルゲートウェイは、AWS Storage Gateway の一つの動作モードです。このモードでは、オンプレミスのアプリケーションがクラウドに保存されたファイルを簡単に利用できるようにします。
  - ローカルキャッシュ：よくアクセスされるデータをオンプレミスにキャッシュすることで、データアクセスの速度を向上させます。
- ファイルの格納：
  - オンプレミスのアプリケーションはファイルを Storage Gateway に送信します（NFS または SMB プロトコルを使用）。
  - Storage Gateway はファイルを Amazon S3 に保存します。
- ファイルの参照：
  - ユーザーがファイルにアクセスしようとすると、まず Storage Gateway にリクエストを送信します。
  - Storage Gateway は Amazon S3 からファイルを取得し、必要に応じてローカルキャッシュからも提供します。
- ボリュームゲートウェイ

  - 用途：ブロックストレージ
    - プロトコル：iSCSI (Internet Small Computer Systems Interface)
    - データ保存：Amazon S3（バックアップ）または完全にオンプレミスに保存
  - 主なユースケース：
    - データベースや仮想マシンのバックアップと復元
    - ディザスタリカバリ
    - オンプレミスとクラウドのブロックストレージ統合
  - 特徴：
    - キャッシュボリューム：オンプレミスに頻繁にアクセスされるデータをキャッシュし、Amazon S3 にバックアップ
    - ストアボリューム：オンプレミスにデータを保存し、Amazon S3 に定期的にスナップショットを取る

- DynamoDB Stream: DynamoDB にデータの更新があった際、Lambda 関数を起動して非同期で処理を行うことができる

- IAM Roles Anywhere: AWS IAM（Identity and Access Management）ロールをオンプレミスのサーバーやアプリケーションから利用できるようにするサービスです。通常、IAM ロールは AWS 内部でしか使用できませんが、このサービスを使うことで、オンプレミス環境からも IAM ロールを安全に使えるようになります。
  - トラストアンカーとは、オンプレミスの環境から AWS のリソースにアクセスするための信頼の基盤です。まず、これを AWS で設定します。
  - 次に、CodeArtifact アクション（操作）を許可する IAM ロールを作成します。このロールには、トラストアンカーと信頼関係を持たせます。
  - 最後に、オンプレミスの CI/CD パイプラインを設定して、この IAM ロールを引き受けるようにします。これにより、オンプレミスから直接 CodeArtifact にアクセスできるようになります。
- AWS CodeArtifact とパブリックリポジトリの連携
  - 1. CodeArtifact リポジトリの作成: CodeArtifact リポジトリは、ソフトウェアパッケージ（依存関係を含む）を保存・管理するための場所です。これを AWS で作成します。
  - 2. 外部接続の設定: 外部接続を使うと、CodeArtifact リポジトリがパブリックリポジトリ（例えば、npm や Maven など）と直接連携できるようになります。
  - 依存リポジトリをアップストリームとして設定: アップストリームとは、依存関係を取得する元となるリポジトリのことです。CodeArtifact リポジトリに対して、必要なパブリックリポジトリ（npm や Maven など）をアップストリームとして設定します。
  - <img width="503" alt="image" src="https://github.com/yoshikikasama/system/assets/61643054/87970e7c-160f-44c8-a4d6-ab250c9c1ead">
  - アップストリームは、依存パッケージを供給する元（パブリックリポジトリなど）です。
  - ダウンストリームは、供給されたパッケージを使用してビルドやデプロイを行う側（自社の CI/CD パイプラインなど）です。

## 63.

- CloudFormation スタックが UPDATE_ROLLBACK_FAILED 状態のままになっている場合:

  - そのスタックで実行できるアクションは ContinueUpdateRollback または DeleteStack のオペレーションのみになります。これは、CloudFormation がユーザーからの追加入力を必要とし、ロールバックしようとしているテンプレートとスタックが同期していないことを確認するためです。ロールバックを再試行してエラーを解決するには、ContinueUpdateRollback を使用します。
  - 解決するには、手動で修正してロールバックする。

- あなたのアプリケーションは、Application Load Balancer（ALB）の背後にある Amazon EC2 インスタンスで実行されています。DevOps エンジニアが AWS CodeDeploy を使用して新しいバージョンをリリースしていますが、デプロイの際に「AllowTraffic」ライフサイクルイベント中に失敗してしまいます。この際、デプロイログにエラーが報告されていません。

  - 解決策: AllowTraffic ライフサイクルイベントが失敗し、デプロイログにエラーが報告されない場合の原因は、多くの場合、ヘルスチェックの設定が間違っているためです。
  - AllowTraffic ライフサイクルイベント: AWS CodeDeploy のライフサイクルイベントの一つで、アプリケーションがトラフィックを受け入れ始めるタイミングを管理します。このイベントが成功すると、ユーザーからのトラフィックが新しいバージョンのアプリケーションにルーティングされます。
  - ALB や他のロードバランサー（Classic Load Balancer や Network Load Balancer）のヘルスチェック設定が正しくない場合、インスタンスは「ヘルシー」と判断されず、トラフィックを受け入れる準備が整っていないとみなされます。

- Kibana では、IAM ユーザーとロールがネイティブにサポートされていませんが、Amazon ES には Kibana へのアクセスを制御するためのいくつかのソリューションが用意されています。
  - Kibana の SAML 認証を有効にします。
  - HTTP 基本認証できめ細かなアクセスコントロールを使用します。
  - 設定 Amazon Cognito Kibana の 認証.
  - パブリックアクセスドメインの場合は、プロキシサーバーのある、または IP ベースのアクセスポリシーを設定します。
  - VPC アクセスドメインの場合、プロキシサーバーのある、またはないオープンアクセスポリシーを使用し、セキュリティグループを使用してアクセスを制御します。詳細については、「VPC ドメインのアクセスポリシーについて」を参照してください。

-　 Amazon OpenSearch Service（旧称 Amazon Elasticsearch Service）: オープンソースの検索および分析エンジンである Elasticsearch をクラウド上で提供するマネージドサービスです。これは、ログの分析、リアルタイムのアプリケーションモニタリング、インフラストラクチャのパフォーマンス分析などに使用されます。

- Kibana は、Elasticsearch のデータを可視化するためのオープンソースのダッシュボードツールです。ユーザーは、Kibana を使用して、インデックスされたデータを検索、表示、および分析するインタラクティブなダッシュボードを作成できます。Amazon OpenSearch Service は、データ検索と分析のためのマネージドサービスであり、Kibana はそのデータを可視化するためのツールです。Kibana へのユーザーアクセスを制限するには、SAML 認証や Amazon Cognito 認証を使用する方法があります。
- ![image](https://github.com/yoshikikasama/system/assets/61643054/00208cba-d705-44b9-ad06-1f01b2b4980b)

- AWS Global Accelerator は、パブリックアプリケーションの可用性、パフォーマンス、およびセキュリティの向上に役立つネットワークサービスです。Global Accelerator は、Application Load Balancer、Network Load Balancer、Amazon Elastic Compute Cloud (EC2) インスタンス、エラスティック IP などのアプリケーションエンドポイントへの固定エントリポイントとして機能する 2 つのグローバル静的パブリック IP を提供します。
- Amazon DynamoDB Accelerator(DAX): DynamoDB と互換性のあるフルマネージド型のインメモリキャッシュサービスになります。
  レスポンスをミリ秒単位からマイクロ秒単位まで高速化することが出来ます。

## 64.

- IAM PassRole: PassRole は、AWS IAM のアクションの一つで、特定の IAM ロールを他の AWS サービスに渡す権限をユーザーに付与するものです。これは、AWS リソースがユーザーに代わってアクションを実行するために重要な役割を果たします。
- なぜ PassRole が必要なのか: たとえば、AWS CloudFormation がスタックを作成・更新する際に、CloudFormation がリソースを操作するための権限を持つ IAM ロールを使用する必要があります。この場合、ユーザーは CloudFormation に対してその IAM ロールを「渡す」必要があります。これを行うために必要なのが PassRole 権限です。

-　ウェブ ACL（Web Access Control List）は、AWS WAF（Web Application Firewall）で使用される設定で、ウェブアプリケーションに対する HTTP および HTTPS リクエストのトラフィックを制御するためのルールセットを定義します。ウェブ ACL は、特定のルールに基づいてリクエストを許可または拒否することができ、セキュリティやアクセス制御を強化するために使用されます。

## 65.

- Service Quotas: 一元的な場所からクォータを表示および管理できる AWS のサービスです。クォータ (制限とも呼ばれます) は、AWS アカウント のリソース、アクション、アイテムの最大値です。Service Quotas が AWS Organizations に関連付けられている場合、クォータリクエストテンプレートを作成して、アカウントの作成時に自動的にクォータの引き上げをリクエストできます。

- 段階的な AWS Lambda デプロイを提供するために CodeDeploy が組み込まれています。

  - Canary: トラフィックは 2 回の増分で移行されます。事前定義された Canary オプションから選択できます。このオプションは、最初の増分で更新された Lambda 関数バージョンに移行されるトラフィックの割合と、2 番目の増分で残りのトラフィックが移行されるまでの間隔を分単位で指定します。
  - Linear: トラフィックは、毎回同じ間隔（分）の等しい増分で移行します。増分ごとに移行されるトラフィックの割合と、増分間の間隔 (分) を指定する事前定義された Linear オプションから選択できます。
  - AllAtOnce: すべてのトラフィックは元の Lambda 関数から最新バージョンの Lambda 関数に一度に移行されます。

- ![image](https://github.com/yoshikikasama/system/assets/61643054/e6043f63-c984-40c1-91e8-41986d0df0b0)
  - 新しいターゲットグループを作成: 各 API エンドポイント（例えば、/api/endpoint1）に対して、Lambda 関数ターゲットグループを作成します。
  - 既存のターゲットグループと新しいターゲットグループの設定: 既存の EC2 インスタンスベースのターゲットグループ（例えば、EC2-Endpoint1）を維持し、新しい Lambda 関数ターゲットグループ（例えば、Lambda-Endpoint1）と併用します。
  - Application Load Balancer (ALB)のリスナールールを設定: ALB のリスナーに、新しいパス条件と重み付けルールを設定します。例えば、/api/endpoint1 に対して以下のような設定を行います：
    - EC2-Endpoint1 ターゲットグループに 80%のトラフィックを割り当てる
    - Lambda-Endpoint1 ターゲットグループに 20%のトラフィックを割り当てる。
  - これにより、新しい Lambda 関数バージョンのテストが限られた数のお客様で行われ、既存の ALB アクセスログに影響を与えず、ログ処理サービスとの互換性も維持されます。

## 66.

- マイクロサービス間の通信をパブリックインターネットを経由せず、プライベートネットワーク接続で行う

  - AWS PrivateLink を使用して、マイクロサービス同士をプライベートに接続する方法を説明します。
  - AWS PrivateLink: VPC 内のサービスを他の VPC やオンプレミスネットワークにプライベートに公開できる機能です。これにより、インターネットを経由せずにサービスを安全に接続できます。
  - ![image](https://github.com/yoshikikasama/system/assets/61643054/0c5648fd-6278-45a0-a2dd-521551099a3a)
  - VPC: 各サービスチームは独自の VPC を持ち、その VPC 内でサービスをホストしています。
    - VPC A: 192.168.0.0/22 CIDR
    - VPC B: 192.168.4.0/22 CIDR
    - VPC C: 192.168.8.0/22 CIDR
  - ALB (Application Load Balancer): 各 VPC 内に配置され、トラフィックを EC2 インスタンスに分散します。
  - EC2 インスタンス (Microservice): 各マイクロサービスをホストしている EC2 インスタンス。
  - NLB (Network Load Balancer): 各 VPC に配置され、AWS PrivateLink のためのエントリーポイントとして機能します。
  - VPC エンドポイントサービス: NLB を指す VPC エンドポイントサービスが各 VPC 内に設定されます。
  - VPC Endpoint Service (A): VPC A からのサービス
  - VPC Endpoint Service (B): VPC B からのサービス
  - VPC Endpoint Service (C): VPC C からのサービス
  - AWS PrivateLink: 各 VPC は、AWS PrivateLink を介して他の VPC とプライベートに接続されます。他の VPC は、エンドポイントサービスに接続するためのインターフェース VPC エンドポイントを作成します。各コンシューマ VPC は、エンドポイントサービスを利用して他のマイクロサービスと通信します。
  - プライベートリンクの通信フロー
    - サービス提供側 (VPC A, B, C): 各 VPC にあるマイクロサービスは NLB を介してエンドポイントサービスとして公開されます。
    - サービス消費側 (Consumer A, B, C): 各 VPC は他の VPC のエンドポイントサービスに接続するためにインターフェース VPC エンドポイントを作成し、プライベートに通信します。
  - なぜ NLB を使用するのか？
    - AWS PrivateLink の要求: AWS PrivateLink は、VPC エンドポイントサービスを作成する際に NLB を必要とします。NLB は、プライベートリンクを介してトラフィックを受け入れるためのエントリーポイントとして機能します。
    - 高性能な TCP トラフィック処理: NLB は、TCP トラフィックの処理に特化しており、高スループットと低レイテンシを提供します。これにより、マイクロサービス間の通信が迅速かつ効率的に行われます。
    - スケーラビリティと可用性: NLB は自動的にスケーリングし、高い可用性を提供します。これにより、トラフィックの増加に対しても対応可能で、サービスの信頼性が向上します。
    - 固定 IP アドレスのサポート: NLB は固定 IP アドレスをサポートしているため、エンドポイントのアドレスが固定され、管理が容易になります。

- Amazon CodeGuru: ML を活用した推奨事項を使用して、コードレビューを自動化し、アプリケーションのパフォーマンスを最適化

  - Amazon CodeGuru Reviewer: アプリケーションの問題やアプリケーションの脆弱性、バグを見つける作業を ML による自動推論で解決するというものです。分析の対象は AWS に連携されたリポジトリサービスであり、CodeCommit や GitHub のリポジトリを連携することによって利用できます。ただし、人によるレビューと同じく、レビューを開始するにはプルリクエストを発行する必要があります。
  - Amazon CodeGuru Profiler: 実稼働しているアプリケーションのパフォーマンスを最適化、および最もコストが高いコード行を特定します。アプリケーションのパフォーマンスの最適化を常に模索しており、CPU 使用率を削減し、計算コストを低減し、アプリケーションのパフォーマンスを向上させるための修正方法を推奨します。アプリケーションのプロファイリングを実行するサービスとして AWS X-Ray というサービスがありますが、AWS X-Ray はサービス間のレイテンシを測定するなどリクエスト周りの測定を実行するサービスであり、マシンスペックに依存するようなところは分析しません。

## 67.

- DevOps エンジニアは、オンプレミスのデータを Amazon S3 に移行するスクリプトを作成しました。このスクリプトは、S3 にデータを転送した後、オンプレミスのデータを削除します。ただし、現在のスクリプトは、データが正しく転送されたかどうかを確認していません。これを確認しないと、転送中にデータが破損しても気づかずにオンプレミスのデータを削除してしまう可能性があります。

  - MD5 チェックサム: データの「指紋」のようなもので、データが変わっていないことを確認するために使われます。同じデータからは必ず同じ MD5 チェックサムが生成されるため、データが転送中に変更されていないかを確認するのに役立ちます。
  - Amazon S3 にデータをアップロードする： アップロード時に、計算した MD5 チェックサムを Content-MD5 ヘッダーとして指定します。これにより、Amazon S3 はデータを受け取った際に自動的にチェックサムを計算し、送信されたものと一致するかを確認します。
  - レスポンスの確認：アップロードが成功した場合、Amazon S3 はレスポンスとして ETag（Entity Tag）を返します。ETag は、特定の条件下でアップロードされたデータの MD5 チェックサムと一致します。
  - MD5 チェックサムの比較：スクリプトは、返された ETag と最初に計算した MD5 チェックサムを比較し、一致するかどうかを確認します。

- RPO（Recovery Point Objective）: データが失われても許容される最大時間。たとえば、RPO が 15 分であれば、最悪でも直近 15 分のデータは失われていないことが要求されます。
- RTO（Recovery Time Objective）: システムやサービスが停止してから復旧するまでの最大許容時間。たとえば、RTO が 4 時間であれば、システム障害から 4 時間以内にサービスが復旧する必要があります。
- 理由: RDS スナップショットの復元には時間がかかるため、15 分の RPO 要件を満たすことができません。スナップショットは定期的に取得されますが、最新のスナップショットと実際の障害発生時のデータとの間にはギャップが生じます。このギャップは RPO を超える可能性が高いです。
- RPO 違反: スナップショット復元にかかる時間とデータのギャップが RPO 要件の 15 分を超えてしまいます。
- RTO 違反: スナップショットの復元プロセス自体が数時間かかる場合があるため、4 時間以内の復旧も難しいです。
- クロスリージョンリードレプリカの作成
  - 目的: 別の AWS リージョンに RDS のリードレプリカを作成し、15 分の RPO を達成します。
  - 手順:
    - メインの AWS リージョンで動作している RDS インスタンスのリードレプリカを別の AWS リージョンに作成します。
    - リードレプリカはメインの RDS インスタンスと継続的に同期されます。

## 68.

- AWS Config 自動修復機能を使用した Amazon S3 バケットのコンプライアンス:
  - s3-bucket-server-side-encryption-enabled AWS Config ルール: S3 バケットで S3 デフォルト暗号化が有効になっているか、または S3 バケットポリシーがサーバー側暗号化なしの put-object リクエストを明示的に拒否しているかどうかを確認します。
  - AWS-EnableS3BucketEncryption: SSM Automation を使用して S3 バケットのサーバー側暗号化を有効にする AWS SSM Automation ドキュメントです。

-　問題の概要

- ある会社が、外部向けの RESTful ウェブサービスを提供しています。このサービスは、短いテキストドキュメントを分析し、その読み取り難易度を返す単一の AWS Lambda 関数で実装されています。
- 現在のリクエスト形式:
- ````{
  "text": "ここにテキストが入る"
  }```
  ````
- 新しいリクエスト形式:
  ```
  {
  "language": "English", // もしくは "Spanish" や "French"
  "text": "ここにテキストが入る"
  }
  ```
  - 要求事項
    - 既存のユーザーは、新しい形式に移行するまでは従来の形式でリクエストを送ることができるようにする。
    - 新しい形式を利用するユーザーは、新しいリクエスト形式を使用する。
    - 本番環境で稼働する Lambda 関数は 1 つだけにする（複数のバージョンが存在しないようにする）。
  - 新しい Lambda 関数をデプロイ: 新しいリクエスト形式（言語とテキスト）に対応するように Lambda 関数を更新します。
  - API Gateway の設定を更新: 既存の API Gateway を、新しい Lambda 関数に接続します。これにより、API Gateway がリクエストを新しい Lambda 関数に渡すようになります。
  - マッピングテンプレートの追加: API Gateway の設定で、マッピングテンプレートを追加します。このテンプレートは、従来のリクエスト形式を新しい形式に変換する役割を果たします。例えば、ユーザーが古い形式のリクエストを送信した場合、API Gateway が自動的に「language: 'English'」を追加するように設定します。
  - API をデプロイ: 更新された設定をデプロイします。これにより、新しい Lambda 関数がすべてのリクエストを処理するようになります。
  - 古い Lambda 関数を削除: 新しい設定が正常に動作していることを確認した後、古い Lambda 関数を削除します。

-　カスタム統合とプロキシ統合の違いを以下の表にまとめます。

| 特徴                 | カスタム統合                                                                              | プロキシ統合                                                                 |
| -------------------- | ----------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------- |
| 設定の柔軟性         | 高い。リクエストとレスポンスのマッピングテンプレートを細かく設定可能。                    | 低い。リクエストとレスポンスはそのまま転送される。                           |
| 設定の手間           | 手間がかかる。各マッピングテンプレートを手動で設定する必要がある。                        | 手間が少ない。設定が簡単。                                                   |
| リクエスト形式       | 自由に変更可能。クライアントからのリクエスト形式をバックエンドに合わせて変換できる。      | 変更不可。クライアントからのリクエストをそのままバックエンドに送信。         |
| レスポンス形式       | 自由に変更可能。バックエンドのレスポンスをクライアントに合わせて変換できる。              | 変更不可。バックエンドのレスポンスをそのままクライアントに返す。             |
| ユースケース         | 特定のリクエスト/レスポンス形式が必要な場合や、複雑なデータ変換が必要な場合に適している。 | 簡単なリクエスト/レスポンスで、特別な変換が不要な場合に適している。          |
| エラーハンドリング   | API Gateway 側で詳細なエラーハンドリングが可能。                                          | バックエンドがエラーハンドリングを担当する。                                 |
| API Gateway の機能   | 多くの API Gateway 機能を活用可能。                                                       | 基本的な API Gateway 機能のみ使用。                                          |
| バックエンドの自由度 | 高い。バックエンドが期待するフォーマットに合わせてリクエストをカスタマイズできる。        | 低い。バックエンドはクライアントからの直接のリクエストを処理する必要がある。 |

- GuardDuty 脅威リスト: このリストには悪意のある既知の IP アドレスが含まれます。
- GuardDuty 信頼できる IP リスト: このリストには、安全な通行として信頼される IP アドレスが含まれます。信頼できる IP リストを設定することで、リスト内の IP アドレスからのトラフィックに対して GuardDuty がアラートを生成しないようにします。

## 69.

- 会社の要件
  - ユーザーに最良の応答時間を提供
  - ユーザーが最も近いリージョン（us-east-1 または eu-west-1）からウェブサービスを利用できるようにする。
  - リージョン障害時のフェイルオーバー
  - 一方のリージョンで障害が発生した場合、もう一方のリージョンにトラフィックを誘導する。
- 解決策

  - サブドメインの設定: us.example.com と eu.example.com のサブドメインを作成し、それぞれのリージョンに対応する ALB を設定。
  - レイテンシーベースルーティングポリシー: example.com の DNS レコードにレイテンシーベースルーティングポリシーを使用して、us.example.com と eu.example.com にトラフィックを振り分ける。
  - フェイルオーバールーティングポリシー: us.example.com と eu.example.com にフェイルオーバールーティングポリシーを設定し、プライマリとセカンダリの ALB を設定。
  - 通常のトラフィックフロー: ユーザーは最も近いリージョンに接続。
    - example.com -> レイテンシーベースルーティング -> us.example.com または eu.example.com
  - フェイルオーバーのトラフィックフロー: リージョン障害時には、もう一方のリージョンにトラフィックが誘導される。
  - <img width="632" alt="image" src="https://github.com/yoshikikasama/system/assets/61643054/14c5e22e-59af-4320-a3f6-2c15c0fab54f">

- 会社の要件
  - アプリケーションは Auto Scaling グループ内の Amazon EC2 インスタンスで実行されている。
  - アプリケーションは、起動後に Amazon S3 バケットからデータを処理する必要がある。
  - データのサイズが増加しており、新しいインスタンスが起動してからリクエストに対応できるまでに時間がかかる。
  - 新しいインスタンスがリクエストに対応できるようになるまでの時間を短縮したい。
- 解決策の概要

  - 新しい EC2 インスタンスが迅速にリクエストに対応できるようにするためには、以下の方法があります：
    - Auto Scaling ウォームプールを使用: ウォームプールを使用すると、インスタンスがすでに起動した状態で待機しているため、追加されたときにすぐにリクエストを処理できます。
      - ウォームプールは、EC2 インスタンスを停止状態で待機させる機能です。これにより、必要なときにすぐにインスタンスを起動させ、データをダウンロードして処理する準備を整えます。ウォームプールの利点は、以下の通りです：
        - 迅速な対応: インスタンスがすでに起動している状態で待機しているため、新しいインスタンスを追加する際に起動時間を節約できます。
        - コスト効率: 停止状態のインスタンスは、ストレージと Elastic IP アドレスの料金のみが発生し、ランニングコストが抑えられます。
      - ライフサイクルフックの使用: Auto Scaling グループにライフサイクルフックを設定し、インスタンスがデータをダウンロードして処理するための時間を確保します。
        - ライフサイクルフックを使用する場合
        - フロー
          - インスタンス起動: Auto Scaling グループが新しいインスタンスを起動し、ライフサイクルフックがトリガーされます。
          - ライフサイクルフックの実行: インスタンスが起動すると、ライフサイクルフックによってインスタンスは「Pending」状態に移行します。この状態では、インスタンスがリクエストを受け取らずにデータのダウンロードを行います。
          - データのダウンロード: インスタンスが Amazon S3 からデータをダウンロードし、必要な準備を行います。
          - ライフサイクルフックの完了: データのダウンロードと準備が完了すると、ライフサイクルフックを完了させてインスタンスを「InService」状態に移行します。
          - リクエストの処理開始: インスタンスがリクエストを受け取り、処理を開始します。

- 会社の問題と要件
  - 問題
    - 注文履歴ページで注文の処理状況の反映に遅れが生じている。
    - 注文処理システムは、AWS Lambda 関数が Amazon SQS キューから注文メッセージを処理し、Amazon DynamoDB テーブルに保存しています。
    - メッセージの処理が遅れ、ユーザーが注文状況を確認するのに時間がかかっている。
  - 要件
    - 処理遅延を減らし、ユーザーが注文状況を迅速に確認できるようにする。
- 解決策のステップ
  - SQS メトリクスの確認と Lambda 関数の同時実行数の制限を増やす
    - ApproximateAgeOfOldestMessage メトリクスを確認し、メッセージ処理の遅延を評価。
      - ApproximateAgeOfOldestMessage メトリクス: SQS キューにある最も古いメッセージがどれだけ長くキューに滞留しているかを示します。
    - 遅延が確認された場合、Lambda 関数の同時実行数の制限を増やす。
  - DynamoDB メトリクスの確認と Auto Scaling ポリシーの調整
    - ThrottledWriteRequests メトリクスを確認し、書き込みが抑制されているか評価。
      - ThrottledWriteRequests メトリクス: DynamoDB テーブルで書き込みが抑制された回数を示します。
    - 書き込みが抑制されている場合、DynamoDB テーブルの Auto Scaling ポリシーを調整し、最大書き込み容量を増やす。
    - SQS リドライブポリシーは、ソースキューとデッドレターキューを指定し、さらにソースキューのコンシューマが一定回数でメッセージ処理に失敗した場合、Amazon SQS がメッセージをソースキューからデッドレターキューへ移動する条件を指定します。回数、失敗についての言及がないため間違いです。

## 70.

- 企業が AWS を使用して以下のタスクを自動化する方法を求めています。
  - Linux AMI の定期的な更新
  - Chef エージェントの新バージョンのインストール
  - 新しい AMI を部門の AWS アカウントに提供
- EC2 Image Builder: 仮想マシン（VM）やコンテナイメージを自動的に構築、テスト、デプロイするためのツールです。これを使うことで、最新かつセキュアなイメージを維持するための作業が簡単になります。
  - 自動化：定期的に AMI イメージを更新するスケジュールを設定できます。
  - 統合：セキュリティパッチやアプリケーションを自動的にイメージに組み込みます。
- AWS Resource Access Manager（RAM）:

  - リソース共有：1 つの AWS アカウントで作成したリソースを他のアカウントと共有可能。
  - セキュリティ：安全にリソースを共有できます。
  - 柔軟性：特定のアカウントや組織単位（OU）とリソースを共有できます。
  - アカウントが組織の一部かどうかにかかわらず、アカウント ID で特定の AWS アカウント と共有することも可能です。

- 会社が Amazon ECS クラスターを使用してワークロードを実行し、アプリケーションログとアクセスログを収集し、ほぼリアルタイムで分析するための最適な手順を見ていきましょう。以下の 3 つの手順が推奨されます。

  - 1. ECS タスク定義のログドライバーを awslogs に変更する
    - ECS タスク定義のログドライバーを awslogs に設定：これにより、アプリケーションログが Amazon CloudWatch Logs に送信されます。
    - Amazon CloudWatch Logs エージェントを ECS インスタンスにインストール：これを行うことで、ECS インスタンス上のアプリケーションからのログを CloudWatch Logs に送信します。
  - 2. ALB のアクセスログを有効にして S3 バケットに送信する
    - ALB のアクセスログを有効にする：これにより、ALB（Application Load Balancer）を通過する全てのリクエストの詳細を記録します。
    - ログ記録用の S3 バケットを設定：ALB のアクセスログを指定した S3 バケットに直接送信します。
      - トラフィック解析：ALB のリクエストデータ（例えば、クライアントの IP アドレスやリクエストパスなど）を収集し、トラフィックパターンを分析できます。
      - セキュリティとパフォーマンスの監視：異常なトラフィックやパフォーマンスの問題を特定するのに役立ちます。
  - 3. Kinesis Data Firehose ストリームを作成し、CloudWatch Logs のサブスクリプションフィルターを設定する
    - Amazon Kinesis Data Firehose ストリームを作成：これにより、データをリアルタイムで取り込み、処理し、指定の宛先（例えば S3 バケット）に配信できます。
    - CloudWatch Logs サブスクリプションフィルターを設定：CloudWatch Logs からのログデータを Kinesis Data Firehose にストリーミングします。

- ブルー/グリーンデプロイメントは、新しいアプリケーションバージョンをリリースする際に、システムの稼働時間を最大化しながらリスクを最小化するための方法です。この方法では、現行のバージョン（ブルー環境）と新しいバージョン（グリーン環境）を別々の環境で実行し、トラフィックを徐々に新しいバージョンに切り替えます。

  - 1. 2 番目の Auto Scaling グループと ALB を作成する
    - 新しい Auto Scaling グループを作成し、ここに新しいバージョンのアプリケーションをデプロイします。
    - 新しい ALB（Application Load Balancer）を作成し、この新しい Auto Scaling グループをターゲットに設定します。
    - ![image](https://github.com/yoshikikasama/system/assets/61643054/9b08baf2-7e2f-4772-bfdf-f00603f8e0de)
  - 2. Route 53 で新しい環境を指す 2 番目のエイリアスレコードを作成し、加重ルーティングポリシーを使用する
    - Amazon Route 53 で、ブルーとグリーンそれぞれの ALB を指す 2 つのエイリアスレコードを作成します。
    - 加重ルーティングポリシーを設定し、ブルー環境からグリーン環境に徐々にトラフィックをシフトさせます。
    - ![image](https://github.com/yoshikikasama/system/assets/61643054/5c1db4c2-118a-4151-9e07-14e925b81e87)
  - 3. プライマリ RDS DB インスタンスを使用するように新しい EC2 インスタンスを設定する
    - 新しい Auto Scaling グループの EC2 インスタンスが、既存のプライマリ RDS DB インスタンスを使用するように設定します。
    - これにより、ブルーとグリーンの両方の環境が同じデータベースを使用することになり、データの一貫性が保たれます。
    - ![image](https://github.com/yoshikikasama/system/assets/61643054/d9272b46-6a57-45a7-b069-38ceccf5f734)

- API Gateway の Canary リリースデプロイの設定:
  - Canary リリースは、新しいバージョンの API (および他のソフトウェア) をテスト目的でデプロイするソフトウェア開発戦略であり、ベースバージョンは同じステージで通常のオペレーション用の本稼働リリースとしてデプロイされたままになります。
  - Canary リリースのデプロイでは、すべての API トラフィックはランダムに区切られて事前に設定された比率で本稼働リリースと Canary リリースに送られます。通常、Canary リリースは低い割合の API トラフィックを受け取り、残りは本稼働リリースが受け取ります。更新された API 機能は、Canary を介した API トラフィックのみに認識されます。Canary トラフィックの割合を調整してテストカバレッジやパフォーマンスを最適化できます。
  - Canary トラフィックを低く保ち、選択をランダムにすることにより、どのような時でもほとんどのユーザーは新しいバージョンの潜在的なバグに悪影響を受けず、また、常に悪影響を受け続けるユーザーもいません。テストメトリクスが要件を満たしたら、Canary リリースを本稼働リリースに昇格させ、Canary をデプロイから無効にします。これにより、本稼働ステージで新機能が使用可能になります。
  - ![image](https://github.com/yoshikikasama/system/assets/61643054/7881c8e1-d925-4ab7-af4c-31ec307f2b94)
  - API Gateway の Canary リリースデプロイを使用すると、新しいアプリケーションバージョンを少数のユーザーに徐々に公開し、問題があれば迅速にロールバックできます。
  - ユーザーへの中断を最小限に抑え、システムの安定性を確保しつつ、新しいバージョンを安全にリリースできます。
  - 新しいバージョン(ステージ)を作成するイメージ。

# Amazon Bedroock 生成 AI アプリ開発入門

## 前提

[ソースコード](https://github.com/minorun365/bedrock-book)

## 第 1 章 生成 AI とは

- 人工知能(AI): 知能を持った人間だからこそ行えるような高度な作業(文書の作成やデータの分析など)を人工的に実現する技術や研究分野
- 生成 AI: 様々なコンテンツを生成できる AI の総称
- 機械学習: 与えられたデータを機械(コンピュータ)が自動で学習し、データの背景にある法則を見つけ出します。こうして学習した法則を元にデータを分類したり、未知のデータを予測したりできます。
- 深層学習: 人間の脳の神経回路を模した仕組みを用いて学習を行う手法。データの入力と出力を直接関係づけるのではなく、その間に中間層という構造を多く設けることから深層学習と呼ばれている。コンピューターがあるデータを学習する上で手掛かり(特徴量)を人間が設定する必要がありました。深層学習ではこの特徴量を自動で見つけることができます。
- 基盤モデル: 大量のデータで事前にトレーニングされた汎用的なモデル。
- ファインチューニング: データの微調整。
- 大規模言語モデル(LLM): 基盤モデルの一つで、言語ベースの処理を汎用的にこなすもの。LLM は入力されたテキストの単語の組み合わせをもとにして、その後に続く確率が高い次の単語を予測します。
- パラメーター: 深層学習におけるモデルの挙動を決定するための変数。パラメーターの数が大きいほど推論性能が上がります。(例；70B= 70 billion、700 億パラメータを表す)
  - 小さすぎると十分な性能が出ない一方で大きすぎると学習や推論に必要な計算リソースが増大するなどのデメリットもあります。
- トークン:　生成 AI のモデルがデータを処理する単位。
- 埋め込み(Embedding): 機械学習の用語でテキストや画像などのデータをベクトル(大きさと向きを持つ量)で表現したものを指す。ベクトル数値を用いた回答により自然言語での意味検索ができる。
- チャンク: 生成 AI のモデルに入力として渡すドキュメントの塊。チャンクが大きすぎると複数のトピックを含む長い文章をベクトルに変換する過程でニュアンスが失われる。Microsft の研究では 512 トークンで分割した場合に検索精度が最も高くなったという検証結果もある。

## 第 3 章 生成 AI アプリの開発手法

- Langchain: 生成 AI フレームワーク。Bedrock で利用できる生成 AI モデルや OpenAI 社の GPT-4、Google 社の Gemini など、さまざまな AI モデルを同じインタフェースで利用できる。
- LlamaIndex: 生成 AI アプリの動作に必要なデータの統合を得意としているため、RAG のユースケースでよく利用される。
- Gradio: プロトタイピング用のインタフェースの作成に使用できるフレームワーク。

## 第 4 章 社内文書検索 RAG をアプリを作ってみよう

- RAG: Retrieval-Augmented Generation。　検索によって拡張された生成の略語であり、LLM が事前学習していない外部情報（最新情報や社内情報など）のドキュメントを検索して取得し、LLM への入力に加えることによって、回答生成の品質を向上させる手法。
- ハルシネーション：　不正確な回答をまるで事実であるかのように生成してしまうこと。
- セマンティック検索（意味検索）：意味的に似た文章同士を近いベクトルとして比較できるようになり、プログラムでも意味に基づいた検索を行う。ドキュメントの埋め込みを行う際は、ドキュメント全体を単一のベクトルへ変換するのではなく、意味的なかたまりを考慮して段落ごとに分けます。この塊をチャンクと呼びます。
- 埋め込みモデル：ユーザーの質問をベクトルへ変換する。
- AWS における RAG アーキテクチャの実装例

  - アプリケーション：質問から回答までの一連の処理を制御する。
    - Amazon ECS, AWS Lambda
  - 埋め込みモデル：ユーザーの質問をベクトルへ変換する
    - Bedrock の Cohere Embed Multilingual モデル
  - ベクトル DB：検索対象のドキュメントをベクトルとして保存する。
    - Amazon OpenSearch Serverless, Amazon Aurora
  - LLM：質問と関連情報をもとに、ユーザーへの最終的な回答を生成する。
    - Anthropi Claude シリーズ

- Amazon OpenSearch Service: オープンソースの検索スイート「OpenSearch」を AWS 上でマネージドに提供するサービス。検索エンジンとインデックスを内包した検索サービスという位置付けの AWS サービスですが、ベクトルデータの保存・検索にも利用可能。
- 全文検索：特定のキーワードの有無を文書の先頭から文末までくまなく探索する。
- セマンティック検索：埋め込みによるベクトル同志の比較などを活用して、検索クエリとなる単語や文が意味的に近い部分を文書から探します。
- ハイブリッド検索：全文検索とセマンティック検索を組み合わせ双方の結果の良いところ取りを行う。
- ニューラル検索：クエリのベクトル変換と LLM による回答生成を完結させることができる。

- Amazon Aurora: PostgreSQL の pgvector というベクトルデータを取り扱うための拡張異能で、RAG 用のベクトル DB としても利用可能。
- Amazon Document DB: IVFFlat および HNSW というベクトル検索に対応。
- Pincone: マネージドなベクトル DB サービス。

- Amazon Kendra: AWS が提供する検索サービス。社内にあるさまざまな種類のデータソースを横断して検索できます。
- S3: S3 をベクトルデータの置き場として使用したサーバレスアプリケーションもある。

  - [Building a serverless document chat with AWS Lambda and Amazon Bedrock](https://aws.amazon.com/jp/blogs/compute/building-a-serverless-document-chat-with-aws-lambda-and-amazon-bedrock/)

- データソースとの接続性重視：
  - Knowledge base を利用する場合、データソースは S3 Bucket に配置する必要があるため、検索対象にしたいプラットフォームによっては S3 Bucket への配置がしづらい場合があります。その場合は Kendra を使用します。Kendra は Web クローラーや AWS 内外の多数のデータソースに対応したコネクタがあります。Kendra では、Retrieve API を利用して RAG に利用する情報を検索しますが、取得可能なデータが検索結果 1 件あたり最大 200 トークンと情報量がやや少なめなので、回答品質や取得内容を確認する。Kendra は比較的高額。
- Open Search Serverless も比較的高額。Pincone は低コストで利用できる。

- RAG の回答品質を上げるための工夫：
  - チャンクサイズの調整：ドキュメントをベクトルに変換する際、埋め込みモデルのトークン上限に応じて事前にチャンクという塊へ分割する必要があります。
  - メタデータの追加：組み込むドキュメントにメタデータ（ファイル名や作成日時など）を付与しておき、RAG でチャンクを取得する際に特定のメタデータでフィルタリングを行うことで関連性の高い情報のみを絞り込む工夫が可能です。
  - リランク：RAG の処理の中で取得された検索結果をユーザーの質問との関連性が高い順に並べることで、回答の品質を高める手法。
  - RAG フュージョン：ユーザーの質問をそのままベクトル変換してドキュメントを取得するのではなく、少し視点を変えた複数の質問に LLM などで変換し、それらのクエリ結果を全て集約し、リランクして最終的な回答を生成する手法。
  - Rewrite-Retrieve-Read: ユーザーの質問をそのまま使ってドキュメント検索するのではなく、LLM などを用いて検索に適したクエリに変換してから RAG を行う。
  - 質問：年休の申請どうすればいい？
  - 検索クエリ：有給休暇　申請
  - HYDE(Hypothetical Document Embeddings): 仮説的なドキュメント埋め込み。ユーザーの質問をそのままドキュメント検索に使わずに、仮の回答を一旦 LLM に出力させてからそれを用いて検索する手法。
    - 質問：聖徳太子の好物はなんでしたか？
    - 検索クエリ：聖徳太子はマカロンを好んだとされています。→ ベクトル DB を検索。
- RAG アプリケーション評価ツール：
  - Regas: RAG パイプラインを評価するためのフレームワーク。テストデータの生成、メトリクス(評価尺度)に基づく RAG アプリケーションの評価、運用中アプリケーションの品質監視など。
  - LangSmith: LangChain 社が提供する LLM アプリケーション開発者向けのプラットフォーム。GUI を通じて tracing, 評価、モニタリングなどを行うことができる。
  - Langfuse: LLM アプリケーションのデバッグ・分析・改善に役立つプラットフォーム。ソースコード公開されており、セルフホスト可能。
    - Docker コンテナを利用できるサーバーと PostgreSQL の DB の準備が必要。

## 第 5 章 便利な自律型エージェントを作ろう

- ReAct: AI エージェントが「自身の行動と理由を推論すること」(Reasoning)と「その推論に基づいて行動すること」(Acting)を組み合わせてユーザーの指示や質問に対応する手法。

- Agents for Amazon Bedrock: ナレッジベースを介した RAG やアクショングループ(Lambda)の実行、および文章の生成を組み合わせた機能を容易に構築できる。
  - AI エージェント：目標（ユーザーの指示）の達成に向けてどのような行動が必要であるか、思考の連鎖(Chain-of-Thought)によって推論します。
  - Lambda: AWS 最新情報を web crawler で取得するなど function calling を実装。
  - ナレッジベース: 社内データ蓄積して回答。
- AI エージェントのユースケース:
  - カスタマーサポート
  - 今人アシスタント
  - ヘルスケア：健康アドバイス
  - 教育支援：生徒に対する個別学習支援や質疑応答
  - コンテンツ制作：文書の書き起こし、要約、構成など
  - クリエイティブ支援：作詞、作曲、デザインなど
  - ゲーム＆エンターティメント：AI キャラクターやチャットなど
  - 翻訳：自動翻訳
  - データ分析：市場調査や製品レビューの自然言語での分析
  - プログラミング：issue から自動でタスク処理

## 第 6 章 Bedrock の機能を使いこなそう

- ファインチューニング:
  - 特定のタスクの精度を高めることを目的とする。「ラベル」(期待される出力データ)付きのデータセットを用いる。
  - Bedrock ではトレーニング用の入力データを JSON 形式で S3 Bucket に配置し、微調整ジョブを実行する。
- 継続的な事前トレーニング:
  - モデルに特定領域の知識を与えることを目的とする。ラベルなしのデータセットを用いる。
  - ファインチューニングよりさらに大量のデータが必要。少なくとも 10 億トークン以上。

## 第 9 章 生成 AI スタック

- エンドユーザー向けのサービス：生成 AI をアプリケーションとして利用したい
- 開発者向けのサービス：生成 AI アプリケーションを作りたい人向けのサービス
- 機械学習エンジニア向けのサービス：生成 AI モデルの学習・推論インフラが欲しい人向けのサービス

## 10 章 Bedrock 活用事例

- Bedrock の技術課題への対応
- ![image](https://github.com/user-attachments/assets/a0b6da35-6f48-4cab-a0fe-b847add5daae)
  - リリース当初にときおり発生していた課題として、画面移時のタイムアウトがありました。Bedrock がジョブやサービスアイデアを生成する際、AWS AppSync から GraphQL というクエリー言語を用いて、AWS Lambda を介した Claude モデルへの推論リクエストが行われます。このとき、Claude のテキスト生成処理が 30 秒を超えてしまうと、AppSync のタイムアウト制限により、時間内に生成結果を受け取れずに、画面が「読み込み中」状態のままになってしまうという事象が発生していました。この問題に対応するために、アーキテクチャに非同期処理を取り入れました。AppSync からのクエリーで Claude への推論リクエストを送する際、レスポンスの取得は同時に行わず一旦テキストの生成を開始するのみに留めます。そして、Claude が生成したテキストは Amazon DynamoDB に書き込んでおき、そのテーブルの変化を AppSync の GraphQL サブスクリプションを用いて監視します。生成テキストが DynamoDB のテーブルに追加されたら、GraphQL のサブスクリプションによって検知され、非同期でアプリケーションの画面表示に反映されます。この改善によって、AppSync のタイムアウトによりアプリケーションの画面が途中で止まってしまう事象を回避することができました。その後の細かい改善の中では、画面遷移を工夫することにより Claude が呼び出されるタイミングを早めて、ユーザーが説明文を読んでいるうちに生成を開始できるようにするなど、より UX を高めるための工夫を続けています。
